# E10 - Random Forest Performance Review

Read and comment the paper *Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?*

### Reference:
http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf


En el documento "Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?" los autores evalúan 179 clasificadores, de 17 familias y en 121 bases de datos, implementados en Weka, R, C/C++ y Matlab, con el fin de determinar cuáles clasificadores tienen mejor rendimiento. Para llevar a cabo esta tarea, los autores generan el training y el test set de forma aleatoria. Luego, los parámetros son ajustados, seleccionando aquellos que proporcionan la mejor precisión en el train set. Posteriormente, llevan a cabo una 4-fold cross validation utilizando toda la base de datos. Por otro lado, como medidas de rendimiento de los clasificadores los autores utilizaron el average accuracy, el Friedman ranking y el Cohen k. Estos concluyen que los mejores clasificadores son los Bosques Aleatorios, seguidos por las Máquinas de Vectores de Soporte. Dentro de los 25 mejores clasificadores, los mejores son los bosques aleatorios implementados en Caret, librería del software R, y que reportan un nivel de precisión del 82.30%, y las máquinas de vectores de soporte con kernel Guassiano, implementadas en el lenguaje C y que reportan un nivel de precisión del 81.8%. En particular, el parallel random forest (parRF t) que ajusta el parámetro mtry, debería ser usado como clasificador de referencia. De forma general, 7 bosques aleatorios y 5 máquinas de soporte están incluídos dentro de los mejores 20 clasificadores. Por otro lado, los mejores resultados se alcanzan con la librería Caret de R. 
El documento es controversial en el sentido de que ha suscitado diferentes reacciones al respecto. Por un lado, están aquellos que dicen que el documento hace un buen trabajo al cuantificar el impacto de ajustar los parámetros, y que los resultados son consistentes con aquellos encontrados en numerosas competiciones en Kaggle. Pero, por otro lado, están aquellos que sugieren que el documento no muestra más de lo que ya toda la comunidad académica conoce. También, se le podría criticar al documento que, el criterio utilizado para seleccionar el training y el test set podría generar sesgos y altos niveles de varianza, ya que no se cuenta con una partición de los datos estándar, lo cual puede hacer las comparaciones entre experimentos imposible. Sin embargo, el criterio utilizado es eficiente en términos computacionales. También, que debido a que la complejidad de las bases de datos es desconocida, no es posible determinar si el error de clasificación se debe al diseño del clasificador o por dificultades intrínsecas del problema. Sin embargo, dentro de las cosas destacables del documento, se encuentran que compara clasificadores de diferentes familias, lo cual no es usual en este tipo de documentos. El documento utiliza un gran número de bases de datos lo cual hace que los resultados obtenidos se puedan extrapolar a otras bases de datos, cosa que usualmente no ocurre cuando se mide el rendimiento de un modelo en un número pequeño de bases de datos. Una última sugerencia sería presentar los resultados del documento de una forma más amigable e intentar implementar en el análisis otros modelos que están cogiendo cada vez más fuerza y uso (como aquellos desarrollos recientes en Deep Learning). 
