{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "## SVM & Regularization\n",
    "\n",
    "\n",
    "For this homework we consider a set of observations on a number of red and white wine varieties involving their chemical properties and ranking by tasters. Wine industry shows a recent growth spurt as social drinking is on the rise. The price of wine depends on a rather abstract concept of wine appreciation by wine tasters, opinion among whom may have a high degree of variability. Pricing of wine depends on such a volatile factor to some extent. Another key factor in wine certification and quality assessment is physicochemical tests which are laboratory-based and takes into account factors like acidity, pH level, presence of sugar and other chemical properties. For the wine market, it would be of interest if human quality of tasting can be related to the chemical properties of wine so that certification and quality assessment and assurance process is more controlled.\n",
    "\n",
    "Two datasets are available of which one dataset is on red wine and have 1599 different varieties and the other is on white wine and have 4898 varieties. All wines are produced in a particular area of Portugal. Data are collected on 12 different properties of the wines one of which is Quality, based on sensory data, and the rest are on chemical properties of the wines including density, acidity, alcohol content etc. All chemical properties of wines are continuous variables. Quality is an ordinal variable with possible ranking from 1 (worst) to 10 (best). Each variety of wine is tasted by three independent tasters and the final rank assigned is the median rank given by the tasters.\n",
    "\n",
    "A predictive model developed on this data is expected to provide guidance to vineyards regarding quality and price expected on their produce without heavy reliance on volatility of wine tasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_w.assign(type = 'white')\n",
    "data = data.append(data_r.assign(type = 'red'), ignore_index=True)\n",
    "data.tail(5)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.1\n",
    "\n",
    "Show the frecuency table of the quality by type of wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>quality</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>681</td>\n",
       "      <td>638</td>\n",
       "      <td>199</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>20</td>\n",
       "      <td>163</td>\n",
       "      <td>1457</td>\n",
       "      <td>2198</td>\n",
       "      <td>880</td>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "quality   3    4     5     6    7    8  9\n",
       "type                                     \n",
       "red      10   53   681   638  199   18  0\n",
       "white    20  163  1457  2198  880  175  5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=data[\"type\"],     \n",
    "                      columns=[data[\"quality\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, vamos a clasificar manualmente un vino como bueno o malo para las dos bases de datos, la de vinos blancos y la de vinos rojos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')\n",
    "data_r = data_r.assign(type = 'red')\n",
    "data_w = data_w.assign(type = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 13 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "type                    4898 non-null object\n",
      "quality                 4898 non-null category\n",
      "dtypes: category(1), float64(11), object(1)\n",
      "memory usage: 464.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_r.loc[(data_r['quality'] >= 7) & (data_r['quality'] <= 9), 'quality_2'] = 1\n",
    "data_r.loc[(data_r['quality'] >=3) & (data_r['quality'] <= 6), 'quality_2'] = 0\n",
    "#data_r\n",
    "data_w.loc[(data_w['quality'] >= 7) & (data_w['quality'] <= 9), 'quality_2'] = 1\n",
    "data_w.loc[(data_w['quality'] >=3) & (data_w['quality'] <= 6), 'quality_2'] = 0\n",
    "#data_w\n",
    "\n",
    "data_r=data_r.drop(['quality'], axis = 1)\n",
    "data_r.rename(columns={'quality_2':'quality'}, inplace=True)\n",
    "data_r['quality'] = data_r['quality'].astype('category')\n",
    "#data_r\n",
    "\n",
    "data_w=data_w.drop(['quality'], axis = 1)\n",
    "data_w.rename(columns={'quality_2':'quality'}, inplace=True)\n",
    "data_w['quality'] = data_w['quality'].astype('category')\n",
    "#data_w\n",
    "data_w.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.2\n",
    "\n",
    "* Standarized the features (not the quality)\n",
    "* Create a binary target for each type of wine\n",
    "* Create two Linear SVM's for the white and red wines, repectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a binary target for each type of wine\n",
    "\n",
    "X_r = data_r.drop(['quality', 'type'], axis = 1)\n",
    "y_r = data_r['quality']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y_r = LabelEncoder()\n",
    "y_r = labelencoder_y_r.fit_transform(y_r)\n",
    "y_r\n",
    "\n",
    "X_w = data_w.drop(['quality', 'type'], axis = 1)\n",
    "y_w = data_w['quality']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y_w = LabelEncoder()\n",
    "y_w = labelencoder_y_w.fit_transform(y_w)\n",
    "y_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Test & Train datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r, y_r, test_size = 0.3, random_state = 0)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarized the features (not the quality)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_r = sc.fit_transform(X_train_r)\n",
    "X_test_r = sc.transform(X_test_r)\n",
    "\n",
    "X_train_w = sc.fit_transform(X_train_w)\n",
    "X_test_w = sc.transform(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create two linear SVM's for the white and red wines, repectively.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier_r = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_r.fit(X_train_r, y_train_r)\n",
    "\n",
    "classifier_w = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_w.fit(X_train_w, y_train_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.3\n",
    "\n",
    "Test the two SVM's using the different kernels (‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los resultados son :\n",
      "\n",
      "\n",
      "Accuracy score del SVM con kernel-rbf en base de vinos rojos : 0.8678281710914455\n",
      "Accuracy score del SVM con kernel-poly en base de vinos rojos : 0.8597762269222446\n",
      "Accuracy score del SVM con kernel-sigmoid en base de vinos rojos : 0.8266836368606281\n",
      "\n",
      "\n",
      "Accuracy score del SVM con kernel-rbf en base de vinos blancos : 0.831992931228562\n",
      "Accuracy score del SVM con kernel-poly en base de vinos blancos : 0.8054282151748617\n",
      "Accuracy score del SVM con kernel-sigmoid en base de vinos blancos : 0.7403666280873835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#red wines\n",
    "    #rbf\n",
    "classifier_r_rbf = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_r_rbf.fit(X_train_r, y_train_r)\n",
    "y_pred_r_rbf = classifier_r_rbf.predict(X_test_r)\n",
    "\n",
    "accuracies_r_rbf = cross_val_score(estimator = classifier_r_rbf, X = X_train_r,\n",
    "                             y = y_train_r, cv = 10)\n",
    "\n",
    "    #poly\n",
    "classifier_r_poly = SVC(kernel = 'poly', random_state = 0)\n",
    "classifier_r_poly.fit(X_train_r, y_train_r)\n",
    "y_pred_r_poly = classifier_r_poly.predict(X_test_r)\n",
    "\n",
    "accuracies_r_poly = cross_val_score(estimator = classifier_r_poly, X = X_train_r,\n",
    "                             y = y_train_r, cv = 10)\n",
    "\n",
    "   #sigmoid\n",
    "classifier_r_sig = SVC(kernel = 'sigmoid', random_state = 0)\n",
    "classifier_r_sig.fit(X_train_r, y_train_r)\n",
    "y_pred_r_sig = classifier_r_sig.predict(X_test_r)\n",
    "\n",
    "accuracies_r_sig = cross_val_score(estimator = classifier_r_sig, X = X_train_r,\n",
    "                             y = y_train_r, cv = 10)\n",
    "\n",
    "#white wines \n",
    "    #rbf\n",
    "classifier_w_rbf = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_w_rbf.fit(X_train_w, y_train_w)\n",
    "y_pred_w_rbf = classifier_w_rbf.predict(X_test_w)\n",
    "\n",
    "accuracies_w_rbf = cross_val_score(estimator = classifier_w_rbf, X = X_train_w,\n",
    "                             y = y_train_w, cv = 10)\n",
    "\n",
    "    #poly\n",
    "classifier_w_poly = SVC(kernel = 'poly', random_state = 0)\n",
    "classifier_w_poly.fit(X_train_w, y_train_w)\n",
    "y_pred_w_poly = classifier_w_poly.predict(X_test_w)\n",
    "\n",
    "accuracies_w_poly = cross_val_score(estimator = classifier_w_poly, X = X_train_w,\n",
    "                             y = y_train_w, cv = 10)\n",
    "\n",
    "   #sigmoid\n",
    "classifier_w_sig = SVC(kernel = 'sigmoid', random_state = 0)\n",
    "classifier_w_sig.fit(X_train_w, y_train_w)\n",
    "y_pred_w_sig = classifier_w_sig.predict(X_test_w)\n",
    "\n",
    "accuracies_w_sig = cross_val_score(estimator = classifier_w_sig, X = X_train_w,\n",
    "                             y = y_train_w, cv = 10)\n",
    "\n",
    "\n",
    "print(\"Los resultados son :\")\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy score del SVM con kernel-rbf en base de vinos rojos :\"+\" \"+str(accuracies_r_rbf.mean()))\n",
    "print(\"Accuracy score del SVM con kernel-poly en base de vinos rojos :\"+\" \"+str(accuracies_r_poly.mean()))\n",
    "print(\"Accuracy score del SVM con kernel-sigmoid en base de vinos rojos :\"+\" \"+str(accuracies_r_sig.mean()))\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy score del SVM con kernel-rbf en base de vinos blancos :\"+\" \"+str(accuracies_w_rbf.mean()))\n",
    "print(\"Accuracy score del SVM con kernel-poly en base de vinos blancos :\"+\" \"+str(accuracies_w_poly.mean()))\n",
    "print(\"Accuracy score del SVM con kernel-sigmoid en base de vinos blancos :\"+\" \"+str(accuracies_w_sig.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠, implementar un modelo de SVM con un kernel rbf tiene el mejor accuracy en ambas bases de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opci√≥n es utilizar la clase GridSearchCV. GridSearchCV implementa un modelo de clasificaci√≥n como cualquier otro, excepto que los par√°metros del clasificador usado para predecir es optimizado por cross validation. A continuaci√≥n, utilizaremos esta opci√≥n para la base de datos de vinos blancos. Observaremos que el accuracy score es el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8319719953325554"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier_w = SVC(random_state = 0)\n",
    "parameters_w = [{'kernel': ['rbf', 'poly', 'sigmoid']}]\n",
    "grid_search_w = GridSearchCV(estimator = classifier_w,\n",
    "                           param_grid = parameters_w,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,)\n",
    "grid_search_w.fit(X_train_w, y_train_w)\n",
    "best_accuracy_w = grid_search_w.best_score_\n",
    "best_parameters_w = grid_search_w.best_params_\n",
    "#here is the best accuracy \n",
    "best_accuracy_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and here is best parameters\n",
    "best_parameters_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4\n",
    "Using the best SVM find the parameters that gives the best performance\n",
    "\n",
    "'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.872207327971403"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid search for best model and parameters\n",
    "\n",
    "    #red wines\n",
    "parameters_r = [{'C': [0.1,1, 10, 100, 1000], 'kernel': ['rbf'],\n",
    "               'gamma': [0.01, 0.001, 0.0001]}]\n",
    "grid_search_r = GridSearchCV(estimator = classifier_r_rbf,\n",
    "                           param_grid = parameters_r,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,)\n",
    "grid_search_r.fit(X_train_r, y_train_r)\n",
    "best_accuracy_r = grid_search_r.best_score_\n",
    "best_parameters_r = grid_search_r.best_params_\n",
    "#here is the best accuracy \n",
    "best_accuracy_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and here is best parameters\n",
    "best_parameters_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8281796966161027"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #white wines\n",
    "parameters_w = [{'C': [0.1,1, 10, 100, 1000], 'kernel': ['rbf'],\n",
    "               'gamma': [0.01, 0.001, 0.0001]}]\n",
    "grid_search_w = GridSearchCV(estimator = classifier_w_rbf,\n",
    "                           param_grid = parameters_w,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,)\n",
    "grid_search_w.fit(X_train_w, y_train_w)\n",
    "best_accuracy_w = grid_search_w.best_score_\n",
    "best_parameters_w = grid_search_w.best_params_\n",
    "#here is the best accuracy \n",
    "best_accuracy_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and here is best parameters\n",
    "best_parameters_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠, es ambos casos el mejor rendimiento est√° dado por los par√°metros C=1000, gamma=0.01 y un kernal rbf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.5\n",
    "\n",
    "Compare the results with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementar√° un modelo de regresi√≥n log√≠stica, para luego comparar el accuracy de estos modelos con aquel de los modelos del ejercicio 6,4 (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695263628239499"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression in red wine dataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_r = LogisticRegression(solver='liblinear',C=1e9)\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "logreg_cv=GridSearchCV(logreg_r,grid,cv=10)\n",
    "logreg_cv.fit(X_train_r,y_train_r)\n",
    "                                        \n",
    "best_accuracy_log_r = logreg_cv.best_score_\n",
    "best_parameters_log_r = logreg_cv.best_params_         \n",
    "                    \n",
    "#here is the best accuracy \n",
    "best_accuracy_log_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and here is best parameters\n",
    "best_parameters_log_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8060093348891482"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression in white wine dataset\n",
    "logreg_w = LogisticRegression(solver='liblinear',C=1e9)\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "logreg_cv=GridSearchCV(logreg_w,grid,cv=10)\n",
    "logreg_cv.fit(X_train_w,y_train_w)\n",
    "                                        \n",
    "best_accuracy_log_w = logreg_cv.best_score_\n",
    "best_parameters_log_w = logreg_cv.best_params_         \n",
    "                    \n",
    "#here is the best accuracy \n",
    "best_accuracy_log_w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and here is best parameters\n",
    "best_parameters_log_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, en ambos casos el accuracy es mejor cuando se implementa un modelo de SVM comparado con aquel cuando se implementa una regresi√≥n logistica "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.6\n",
    "\n",
    "\n",
    "* Train a linear regression to predict wine quality (Continous)\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "quality                 4898 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')\n",
    "data_w.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets X y Y\n",
    "X_r = data_r.drop(['quality'], axis = 1)\n",
    "y_r = data_r['quality']\n",
    "\n",
    "X_w = data_w.drop(['quality'], axis = 1)\n",
    "y_w = data_w['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test & Train data\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r, y_r, test_size = 0.3, random_state = 0)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarized the features (not the quality)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_r = sc.fit_transform(X_train_r)\n",
    "X_test_r = sc.transform(X_test_r)\n",
    "\n",
    "X_train_w = sc.fit_transform(X_train_w)\n",
    "X_test_w = sc.transform(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03523454 -0.22500299 -0.01917881  0.03301609 -0.09404393  0.02142016\n",
      " -0.10287699 -0.03175893 -0.06166729  0.15260506  0.28471337]\n",
      "[ 0.09636772 -0.18212585 -0.00178702  0.48284855 -0.00936514  0.0871764\n",
      " -0.01019314 -0.5914199   0.13181841  0.07656047  0.15829533]\n"
     ]
    }
   ],
   "source": [
    "#Train a linear regression to predict wine quality (Continous).\n",
    "import statsmodels.api as sm\n",
    "#red wines\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg_r = LinearRegression()\n",
    "linreg_r.fit(X_train_r, y_train_r)\n",
    "print(linreg_r.coef_)\n",
    "y_pred_r = linreg_r.predict(X_test_r)\n",
    "\n",
    "#white wines\n",
    "linreg_w = LinearRegression()\n",
    "linreg_w.fit(X_train_w, y_train_w)\n",
    "print(linreg_w.coef_)\n",
    "y_pred_w = linreg_w.predict(X_test_w)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados muestran lo siguiente:\n",
    "1. En ambos casos, un aumento de la acidez fija implica un incremento de la calidad del vino.\n",
    "2. En ambos casos, un aumento de la acidez volatil implica una disminuci√≥n de la calidad del vino.\n",
    "3. En ambos casos, un aumento del √°cido c√≠trico implica una disminuci√≥n de la calidad del vino.\n",
    "4. En ambos casos, un aumento del az√∫car residual implica un incremento de la calidad del vino.\n",
    "5. En ambos casos, un aumento del cloruro implica una disminuci√≥n de la calidad del vino.\n",
    "6. En ambos casos, un aumento del di√≥xido de azufre libre implica un aumento de la calidad del vino.\n",
    "7. En ambos casos, un aumento del di√≥xido de azufre total implica una disminuci√≥n de la calidad del vino.\n",
    "8. En ambos casos, un aumento de la densidad implica una disminuci√≥n de la calidad del vino.\n",
    "9. Si el vino es rojo, un aumento del pH implica una disminuci√≥n de la calidad del vino. Mientras que si el vino es blanco, un aumento del pH implica un aumento de la calidad del vino.\n",
    "10. En ambos casos, un aumento del sulfato implica un aumento de la calidad del vino.\n",
    "11. En ambos casos, un aumento del alcohol implica un aumento de la calidad del vino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE de la regresi√≥n lineal en la base de vinos rojos : 0.6330721652193917\n",
      "RMSE de la regresi√≥n lineal en la base de vinos blancos : 0.7797679548193168\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the RMSE\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "print(\"RMSE de la regresi√≥n lineal en la base de vinos rojos :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r))))\n",
    "print(\"RMSE de la regresi√≥n lineal en la base de vinos blancos :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.7\n",
    "\n",
    "* Estimate a ridge regression with alpha equals 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate a ridge regression with alpha equals 0.1 and 1.\n",
    "\n",
    "#red wines\n",
    "    #alpha 0.1 \n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridgereg_r_01 = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg_r_01.fit(X_train_r, y_train_r)\n",
    "y_pred_r_01 = ridgereg_r_01.predict(X_test_r)\n",
    "   \n",
    "    #alpha 1.0 \n",
    "ridgereg_r_1 = Ridge(alpha=1, normalize=True)\n",
    "ridgereg_r_1.fit(X_train_r, y_train_r)\n",
    "y_pred_r_1 = ridgereg_r_1.predict(X_test_r)\n",
    "\n",
    "\n",
    "#white wines\n",
    "    #alpha 0.1 \n",
    "\n",
    "ridgereg_w_01 = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg_w_01.fit(X_train_w, y_train_w)\n",
    "y_pred_w_01 = ridgereg_w_01.predict(X_test_w)\n",
    "   \n",
    "    #alpha 1.0 \n",
    "ridgereg_w_1 = Ridge(alpha=1, normalize=True)\n",
    "ridgereg_w_1.fit(X_train_w, y_train_w)\n",
    "y_pred_w_1 = ridgereg_w_1.predict(X_test_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04671119 -0.20098627  0.0110126   0.03533417 -0.08771638  0.01206531\n",
      " -0.09294553 -0.05696875 -0.03941277  0.14213138  0.24847562]\n",
      "\n",
      "\n",
      "[ 0.03198728 -0.12515816  0.04854226  0.01505283 -0.05036445 -0.00898519\n",
      " -0.05605514 -0.05090195 -0.01553189  0.0819124   0.1500815 ]\n",
      "\n",
      "\n",
      "[-0.00705278 -0.1649712   0.0011319   0.17096024 -0.03894095  0.0937976\n",
      " -0.03558273 -0.15091429  0.04693054  0.04856757  0.31119153]\n",
      "\n",
      "\n",
      "[-0.02264332 -0.08658794  0.00600274  0.03011663 -0.05449677  0.04654489\n",
      " -0.03102156 -0.07614674  0.02447019  0.02576662  0.15879778]\n"
     ]
    }
   ],
   "source": [
    "#Compare the coefficients with the linear regression\n",
    "\n",
    "print(ridgereg_r_01.coef_)\n",
    "print(\"\\n\")\n",
    "print(ridgereg_r_1.coef_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(ridgereg_w_01.coef_)\n",
    "print(\"\\n\")\n",
    "print(ridgereg_w_1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado lo anterior, se puede concluir de forma general que cuando el alfa es igual a 0,1 los resultados son similares a aquellos encontrados cuando se implement√≥ una regresi√≥n lineal, mientras que cuando el alfa es igual a 1, la penalizaci√≥n es mayor y por lo tanto, la magnitud de los coeficientes son menores al caso de cuando se implement√≥ una regresi√≥n lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE de la regresi√≥n de ridge en la base de vinos rojos con alfa igual a 0,1 : 0.6344194631893529\n",
      "RMSE de la regresi√≥n de ridge en la base de vinos rojos con alfa igual a 1 : 0.6569663486954711\n",
      "RMSE de la regresi√≥n de ridge en la base de vinos blancos con alfa igual a 0,1 : 0.7797233024279017\n",
      "RMSE de la regresi√≥n de ridge en la base de vinos blancos con alfa igual a 1 : 0.8120347448335604\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the RMSE\n",
    "\n",
    "print(\"RMSE de la regresi√≥n de ridge en la base de vinos rojos con alfa igual a 0,1 :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r_01))))\n",
    "print(\"RMSE de la regresi√≥n de ridge en la base de vinos rojos con alfa igual a 1 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r_1))))\n",
    "print(\"RMSE de la regresi√≥n de ridge en la base de vinos blancos con alfa igual a 0,1 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w_01))))\n",
    "print(\"RMSE de la regresi√≥n de ridge en la base de vinos blancos con alfa igual a 1 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w_1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.8\n",
    "\n",
    "* Estimate a lasso regression with alpha equals 0.01, 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate a ridge regression with alpha equals 0.01, 0.1 and 1.\n",
    "\n",
    "#red wines \n",
    "    \n",
    "    #alpha 0.01\n",
    "    \n",
    "from sklearn.linear_model import Lasso\n",
    "lassoreg_r_001 = Lasso(alpha=0.01, normalize=True)\n",
    "lassoreg_r_001.fit(X_train_r, y_train_r)\n",
    "y_pred_r_001 = lassoreg_r_001.predict(X_test_r)\n",
    "\n",
    "    #alpha 0.01\n",
    "\n",
    "lassoreg_r_01 = Lasso(alpha=0.1, normalize=True)\n",
    "lassoreg_r_01.fit(X_train_r, y_train_r)\n",
    "y_pred_r_01 = lassoreg_r_01.predict(X_test_r)\n",
    "\n",
    "    #alpha 1\n",
    "\n",
    "lassoreg_r_1 = Lasso(alpha=1, normalize=True)\n",
    "lassoreg_r_1.fit(X_train_r, y_train_r)\n",
    "y_pred_r_1 = lassoreg_r_1.predict(X_test_r)\n",
    "\n",
    "#white wines \n",
    "    \n",
    "    #alpha 0.01\n",
    "    \n",
    "lassoreg_w_001 = Lasso(alpha=0.01, normalize=True)\n",
    "lassoreg_w_001.fit(X_train_w, y_train_w)\n",
    "y_pred_w_001 = lassoreg_w_001.predict(X_test_w)\n",
    "\n",
    "    #alpha 0.01\n",
    "\n",
    "lassoreg_w_01 = Lasso(alpha=0.1, normalize=True)\n",
    "lassoreg_w_01.fit(X_train_w, y_train_w)\n",
    "y_pred_w_01 = lassoreg_w_01.predict(X_test_w)\n",
    "\n",
    "    #alpha 1\n",
    "\n",
    "lassoreg_w_1 = Lasso(alpha=1, normalize=True)\n",
    "lassoreg_w_1.fit(X_train_w, y_train_w)\n",
    "y_pred_w_1 = lassoreg_w_1.predict(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.00601074  0.          0.         -0.         -0.\n",
      " -0.         -0.         -0.          0.          0.04489403]\n",
      "\n",
      "\n",
      "[ 0. -0.  0.  0. -0. -0. -0. -0. -0.  0.  0.]\n",
      "\n",
      "\n",
      "[ 0. -0.  0.  0. -0. -0. -0. -0. -0.  0.  0.]\n",
      "\n",
      "\n",
      "[-0. -0. -0. -0. -0.  0. -0. -0.  0.  0.  0.]\n",
      "\n",
      "\n",
      "[-0. -0. -0. -0. -0.  0. -0. -0.  0.  0.  0.]\n",
      "\n",
      "\n",
      "[-0. -0. -0. -0. -0.  0. -0. -0.  0.  0.  0.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the coefficients with the linear regression\n",
    "\n",
    "print(lassoreg_r_001.coef_)\n",
    "print(\"\\n\")\n",
    "print(lassoreg_r_01.coef_)\n",
    "print(\"\\n\")\n",
    "print(lassoreg_r_1.coef_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(lassoreg_w_001.coef_)\n",
    "print(\"\\n\")\n",
    "print(lassoreg_w_01.coef_)\n",
    "print(\"\\n\")\n",
    "print(lassoreg_w_1.coef_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El anterior resultado muestra que incluso cuando la penalizaci√≥n es peque√±a (igual a 0,01), la regresi√≥n de lasso convierte a 0 los coeficientes de casi todas las variables, removi√©ndolas as√≠ del modelo, a comparaci√≥n del modelo de regresi√≥n lineal donde ning√∫n coeficiente es igual a 0, y del modelo de ridge donde los coeficientes son peque√±os y m√°s cercanos a 0 pero nunca iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 0,01 : 0.7463573531910372\n",
      "RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 0,1 : 0.7698374031730867\n",
      "RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 1 : 0.7698374031730867\n",
      "RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 0,01 : 0.9016157829038678\n",
      "RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 0,1 : 0.9016157829038678\n",
      "RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 1 : 0.9016157829038678\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the RMSE\n",
    "\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 0,01 :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r_001))))\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 0,1 :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r_01))))\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos rojos con alfa igual a 1 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_r, y_pred_r_1))))\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 0,01 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w_001))))\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 0,1 :\"+\" \"+str(np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w_01))))\n",
    "print(\"RMSE de la regresi√≥n de lasso en la base de vinos blancos con alfa igual a 1 :\"+\" \"+str (np.sqrt(metrics.mean_squared_error(y_test_w, y_pred_w_1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.9\n",
    "\n",
    "* Create a binary target\n",
    "\n",
    "* Train a logistic regression to predict wine quality (binary)\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a binary target\n",
    "\n",
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')\n",
    "data_r = data_r.assign(type = 'red')\n",
    "data_w = data_w.assign(type = 'white')\n",
    "data_r.loc[(data_r['quality'] >= 7) & (data_r['quality'] <= 9), 'quality_2'] = 1\n",
    "data_r.loc[(data_r['quality'] >=3) & (data_r['quality'] <= 6), 'quality_2'] = 0\n",
    "#data_r\n",
    "data_w.loc[(data_w['quality'] >= 7) & (data_w['quality'] <= 9), 'quality_2'] = 1\n",
    "data_w.loc[(data_w['quality'] >=3) & (data_w['quality'] <= 6), 'quality_2'] = 0\n",
    "#data_w\n",
    "\n",
    "data_r=data_r.drop(['quality'], axis = 1)\n",
    "data_r.rename(columns={'quality_2':'quality'}, inplace=True)\n",
    "data_r['quality'] = data_r['quality'].astype('category')\n",
    "#data_r\n",
    "\n",
    "data_w=data_w.drop(['quality'], axis = 1)\n",
    "data_w.rename(columns={'quality_2':'quality'}, inplace=True)\n",
    "data_w['quality'] = data_w['quality'].astype('category')\n",
    "#data_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_r = data_r.drop(['quality', 'type'], axis = 1)\n",
    "y_r = data_r['quality']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y_r = LabelEncoder()\n",
    "y_r = labelencoder_y_r.fit_transform(y_r)\n",
    "y_r\n",
    "\n",
    "X_w = data_w.drop(['quality', 'type'], axis = 1)\n",
    "y_w = data_w['quality']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y_w = LabelEncoder()\n",
    "y_w = labelencoder_y_w.fit_transform(y_w)\n",
    "y_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test & Train datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r, y_r, test_size = 0.3, random_state = 0)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_train_r = sc.fit_transform(X_train_r)\n",
    "#X_test_r = sc.transform(X_test_r)\n",
    "\n",
    "#X_train_w = sc.fit_transform(X_train_w)\n",
    "#X_test_w = sc.transform(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a logistic regression to predict wine quality (binary)\n",
    "\n",
    "#red wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_r = LogisticRegression(C=1e9,solver='liblinear')\n",
    "logreg_r.fit(X_train_r, y_train_r)\n",
    "y_pred_r = logreg_r.predict(X_test_r)\n",
    "\n",
    "\n",
    "#white wine\n",
    "logreg_w = LogisticRegression(C=1e9,solver='liblinear')\n",
    "logreg_w.fit(X_train_w, y_train_w)\n",
    "y_pred_w = logreg_w.predict(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.07228790e-01 -4.65023271e+00 -2.28456937e-02  1.38909930e-01\n",
      "  -7.69158260e+00  5.18432033e-03 -1.28377509e-02 -6.84868428e+00\n",
      "   3.09590278e-01  3.10429491e+00  9.56426157e-01]]\n",
      "\n",
      "\n",
      "[[ 6.56445277e-02 -3.94049063e+00 -8.99937341e-01  6.07584200e-02\n",
      "  -1.18131449e+01  1.40757620e-02 -2.85416857e-03 -7.76549929e+00\n",
      "   1.19536881e+00  1.16976609e+00  9.16254534e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Analyze the coefficients\n",
    "\n",
    "print(logreg_r.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los coeficientes de una regresi√≥n log√≠stica no son muy informativos, indican el cambio en el logaritmo de los odds cuando la variable independiente aumenta en una unidad. Por ejemplo, para el caso de los vinos rojos, un aumento de una unidad en la acidez fija implica un incremento de 1,07 en el logaritmo de los odds de la calidad del vino, dejando el resto de las variables fijas. Por esta raz√≥n, es que en lugar de utilizar el coeficiente de la regresi√≥n log√≠stica se utiliza su correspondiente odd ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score en la base de vinos rojos : 0.39999999999999997\n",
      "f1 score en la base de vinos blanos : 0.3257918552036199\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the f1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score en la base de vinos rojos :\"+\" \"+str (f1_score(y_test_r,y_pred_r)))\n",
    "print(\"f1 score en la base de vinos blanos :\"+\" \"+str (f1_score(y_test_w,y_pred_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.10\n",
    "\n",
    "* Estimate a regularized logistic regression using:\n",
    "* C = 0.01, 0.1 & 1.0\n",
    "* penalty = ['l1, 'l2']\n",
    "* Compare the coefficients and the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate a regularized logistic regression\n",
    "\n",
    "#red wines\n",
    "    #C=0,01, penalty l1 and L2\n",
    "    \n",
    "logreg_r_001l1 = LogisticRegression(C=0.01, penalty='l1',solver='liblinear')\n",
    "logreg_r_001l1.fit(X_train_r, y_train_r)\n",
    "y_pred_r_001l1 = logreg_r_001l1.predict(X_test_r)\n",
    "\n",
    "logreg_r_001l2 = LogisticRegression(C=0.01, penalty='l2',solver='liblinear')\n",
    "logreg_r_001l2.fit(X_train_r, y_train_r)\n",
    "y_pred_r_001l2 = logreg_r_001l2.predict(X_test_r)\n",
    "\n",
    "\n",
    "    #C=0,1, penalty l1 and L2\n",
    "    \n",
    "logreg_r_01l1 = LogisticRegression(C=0.1, penalty='l1',solver='liblinear')\n",
    "logreg_r_01l1.fit(X_train_r, y_train_r)\n",
    "y_pred_r_01l1 = logreg_r_01l1.predict(X_test_r)\n",
    "\n",
    "logreg_r_01l2 = LogisticRegression(C=0.1, penalty='l2',solver='liblinear')\n",
    "logreg_r_01l2.fit(X_train_r, y_train_r)\n",
    "y_pred_r_01l2 = logreg_r_01l2.predict(X_test_r)\n",
    "\n",
    "\n",
    "    #C=1, penalty l1 and L2\n",
    "    \n",
    "logreg_r_1l1 = LogisticRegression(C=1, penalty='l1',solver='liblinear')\n",
    "logreg_r_1l1.fit(X_train_r, y_train_r)\n",
    "y_pred_r_1l1 = logreg_r_1l1.predict(X_test_r)\n",
    "\n",
    "\n",
    "logreg_r_1l2 = LogisticRegression(C=1, penalty='l2',solver='liblinear')\n",
    "logreg_r_1l2.fit(X_train_r, y_train_r)\n",
    "y_pred_r_1l2 = logreg_r_1l2.predict(X_test_r)\n",
    "\n",
    "    \n",
    "\n",
    "#white wines\n",
    "    #C=0,01, penalty l1 and L2\n",
    "    \n",
    "logreg_w_001l1 = LogisticRegression(C=0.01, penalty='l1',solver='liblinear')\n",
    "logreg_w_001l1.fit(X_train_w, y_train_w)\n",
    "y_pred_w_001l1 = logreg_w_001l1.predict(X_test_w)\n",
    "\n",
    "\n",
    "logreg_w_001l2 = LogisticRegression(C=0.01, penalty='l2',solver='liblinear')\n",
    "logreg_w_001l2.fit(X_train_w, y_train_w)\n",
    "y_pred_w_001l2 = logreg_w_001l2.predict(X_test_w)\n",
    "\n",
    "\n",
    "    #C=0,1, penalty l1 and L2\n",
    "    \n",
    "logreg_w_01l1 = LogisticRegression(C=0.1, penalty='l1',solver='liblinear')\n",
    "logreg_w_01l1.fit(X_train_w, y_train_w)\n",
    "y_pred_w_01l1 = logreg_w_01l1.predict(X_test_w)\n",
    "\n",
    "\n",
    "logreg_w_01l2 = LogisticRegression(C=0.1, penalty='l2',solver='liblinear')\n",
    "logreg_w_01l2.fit(X_train_w, y_train_w)\n",
    "y_pred_w_01l2 = logreg_w_01l2.predict(X_test_w)\n",
    "\n",
    "\n",
    "    #C=1, penalty l1 and L2\n",
    "    \n",
    "logreg_w_1l1 = LogisticRegression(C=1, penalty='l1',solver='liblinear')\n",
    "logreg_w_1l1.fit(X_train_w, y_train_w)\n",
    "y_pred_w_1l1 = logreg_w_1l1.predict(X_test_w)\n",
    "\n",
    "\n",
    "logreg_w_1l2 = LogisticRegression(C=1, penalty='l2',solver='liblinear')\n",
    "logreg_w_1l2.fit(X_train_w, y_train_w)\n",
    "y_pred_w_1l2 = logreg_w_1l2.predict(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.05312673  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.30036999]]\n",
      "\n",
      "\n",
      "[[ 0.10581735 -0.24410939  0.13874937  0.08748767 -0.11793492 -0.03857878\n",
      "  -0.11303616 -0.16796152  0.00272116  0.20888344  0.38480811]]\n",
      "\n",
      "\n",
      "[[ 0.10454161 -0.69145307  0.          0.04783128 -0.12941093  0.\n",
      "  -0.20035459  0.          0.          0.36373742  0.88213212]]\n",
      "\n",
      "\n",
      "[[ 0.33516164 -0.52464543  0.13527128  0.23952232 -0.25557932 -0.01573835\n",
      "  -0.27817828 -0.36590727  0.10820021  0.46105     0.64638141]]\n",
      "\n",
      "\n",
      "[[ 0.54489943 -0.73078731  0.04599536  0.33931111 -0.31130653  0.\n",
      "  -0.37206908 -0.49938641  0.20192802  0.59483627  0.76365044]]\n",
      "\n",
      "\n",
      "[[ 0.5908664  -0.69332112  0.08077686  0.3629774  -0.32481745  0.01631072\n",
      "  -0.38890623 -0.56147098  0.24227814  0.60677438  0.72006846]]\n",
      "\n",
      "\n",
      "[[ 0.         -0.09706075  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.73745331]]\n",
      "\n",
      "\n",
      "[[ 0.03536665 -0.22689488 -0.02945548  0.24823684 -0.19581123  0.15805258\n",
      "  -0.07475967 -0.27870389  0.14493826  0.10944778  0.64017526]]\n",
      "\n",
      "\n",
      "[[ 0.04992803 -0.37996564 -0.04128331  0.39887189 -0.26689074  0.16256866\n",
      "  -0.01677313 -0.28762396  0.18532265  0.1286786   0.94754952]]\n",
      "\n",
      "\n",
      "[[ 0.20429124 -0.38639962 -0.07468154  0.71668693 -0.28220425  0.19779416\n",
      "  -0.05243517 -0.7597441   0.30157423  0.17251645  0.72400404]]\n",
      "\n",
      "\n",
      "[[ 0.4067276  -0.41553633 -0.07756059  1.28752884 -0.26151428  0.17524773\n",
      "   0.         -1.62523289  0.46576638  0.2270486   0.37755089]]\n",
      "\n",
      "\n",
      "[[ 0.3925671  -0.41836751 -0.08223644  1.23775517 -0.27096416  0.17971614\n",
      "  -0.00408099 -1.54202172  0.45394212  0.22380739  0.41822177]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the coefficients\n",
    "\n",
    "print(logreg_r_001l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_r_001l2.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_r_01l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_r_01l2.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_r_1l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_r_1l2.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_001l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_001l2.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_01l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_01l2.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_1l1.coef_)\n",
    "print(\"\\n\")\n",
    "print(logreg_w_1l2.coef_)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma general, se observa que a medida que la penalizaci√≥n es mayor (valores m√°s peque√±os de C), m√°s peque√±os son los coeficientes,comparados con aquellos de la regresi√≥n log√≠stica del ejercicio 6,9. Adem√°s, cuando la penalizaci√≥n es tipo l1 (lasso) los coeficientes de la mayor√≠a de las variables son 0, cuando la penalizaci√≥n es muy alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score c=0,01 penalty l1 en la base de vinos rojos : 0.0\n",
      "f1 score c=0,01 penalty l2 en la base de vinos rojos : 0.3950617283950617\n",
      "\n",
      "\n",
      "f1 score c=0,1 penalty l1 en la base de vinos rojos : 0.40963855421686746\n",
      "f1 score c=0,1 penalty l2 en la base de vinos rojos : 0.43902439024390244\n",
      "\n",
      "\n",
      "f1 score c=1 penalty l1 en la base de vinos rojos : 0.4367816091954023\n",
      "f1 score c=1 penalty l2 en la base de vinos rojos : 0.4367816091954023\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "f1 score c=0,01 penalty l1 en la base de vinos blancos : 0.20460358056265981\n",
      "f1 score c=0,01 penalty l2 en la base de vinos blancos : 0.2997658079625293\n",
      "\n",
      "\n",
      "f1 score c=0,1 penalty l1 en la base de vinos blancos : 0.3257918552036199\n",
      "f1 score c=0,1 penalty l2 en la base de vinos blancos : 0.32286995515695066\n",
      "\n",
      "\n",
      "f1 score c=1 penalty l1 en la base de vinos blancos : 0.33185840707964603\n",
      "f1 score c=1 penalty l2 en la base de vinos blancos : 0.3274336283185841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Compare the f1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score c=0,01 penalty l1 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_001l1)))\n",
    "print(\"f1 score c=0,01 penalty l2 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_001l2)))\n",
    "print(\"\\n\")\n",
    "print(\"f1 score c=0,1 penalty l1 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_01l1)))\n",
    "print(\"f1 score c=0,1 penalty l2 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_01l2)))\n",
    "print(\"\\n\")\n",
    "print(\"f1 score c=1 penalty l1 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_1l1)))\n",
    "print(\"f1 score c=1 penalty l2 en la base de vinos rojos :\"+\" \"+str(f1_score(y_test_r,y_pred_r_1l2)))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"f1 score c=0,01 penalty l1 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_001l1)))\n",
    "print(\"f1 score c=0,01 penalty l2 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_001l2)))\n",
    "print(\"\\n\")\n",
    "print(\"f1 score c=0,1 penalty l1 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_01l1)))\n",
    "print(\"f1 score c=0,1 penalty l2 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_01l2)))\n",
    "print(\"\\n\")\n",
    "print(\"f1 score c=1 penalty l1 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_1l1)))\n",
    "print(\"f1 score c=1 penalty l2 en la base de vinos blancos :\"+\" \"+str(f1_score(y_test_w,y_pred_w_1l2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un f1 score para la base de vinos rojos es mayor a aquel de la base de vinos blancos, para los tres niveles de penalidad (C). En la base de vinos rojos, para bajos valores de penalizaci√≥n, l2 reporta mejores niveles de precisi√≥n (m√°s cercanos a 1) que aquellos si se implementa una penalizaci√≥n tipo l1. Sin embargo, a medida que la penalizaci√≥n aumenta los niveles de precisi√≥n tienden a ser similares para ambos tipos de penalizaci√≥n. Lo mismo ocurre con la base de vinos blancos. Por otro lado, por default de la funci√≥n, los resultados del ejercicio 6.9 muestran los resultados del f1 score con un C=1 y una penalidad tipo l2. As√≠, estos resultados deben ser comparados con su similar de este ejercicio (ejercicio 6.10). Por lo tanto, para la base de vinos rojos regularizada el f1 score con c=1 y l2 de penalidad es de 0,43, mayor a 0,39, que es el f1 score del ejercicio 6.9. Por su parte, para la base de vinos blancos el f1 score es en ambos casos (ejercicio 6.9 y 6.10) igual a 0.32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
