E8 - Ensemble Trees Overview

Write

Los ensemble methods son una técnica de aprendizaje automático que combina varios modelos básicos para producir un modelo predictivo óptimo. El objetivo de cualquier problema de aprendizaje automático es encontrar un modelo único que pueda predecir mejor nuestro resultado deseado. En lugar de hacer un modelo y esperar que este sea el mejor predictor que podamos tener, los ensemble methods tienen en cuenta una gran cantidad de modelos y promedian esos modelos para producir un modelo final. En particular, los ensemble methods son meta-algoritmos que combinan varias técnicas de aprendizaje automático en un modelo predictivo para disminuir la varianza (bagging), sesgo (boosting) o mejorar las predicciones (stacking). En Bagging, conocido como bootstrapped aggregation, el mismo algoritmo tiene diferentes perspectivas del problema al entrenar los modelos en diferentes subsets del training data. Por lo tanto, reduce la varianza al evitar que los resultados se vean afectados por outliers. Bagging debería ser usado con clasificadores poco estables, es decir, clasificadores que son sensibles a variaciones en el training set, como los árboles de decisión. Boosting está basado en el cuestionamiento planteado por Kearns y Valiant (1988, 1989): ¿Puede un conjunto de clasificadores débiles crear un clasificador robusto?. Un clasificador débil está definido para ser un clasificador el cual está solo débilmente correlacionado con la clasificación correcta (él mismo clasifica mejor que un clasificador aleatorio). En contraste, un clasificador robusto es un clasificador que tiene un mejor desempeño que el de un clasificador débil, ya que sus clasificaciones se aproximan más a las verdaderas clases. El boosting consiste en combinar los resultados de varios clasificadores débiles para obtener un clasificador robusto. Cuando se añaden estos clasificadores débiles, se lo hace de modo que estos tengan diferente peso en función de la exactitud de sus predicciones. Luego de que se añade un clasificador débil, los datos cambian su estructura de pesos: los casos que son mal clasificados ganan peso y los que son clasificados correctamente pierden peso. Así, los clasificadores débiles se centran de mayor manera en los casos que fueron mal clasificados por los clasificadores débiles. De esta forma, boosting reduce el sesgo de las estimaciones.